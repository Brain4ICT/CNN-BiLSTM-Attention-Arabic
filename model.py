# -*- coding: utf-8 -*-
"""CNN-BiLSTM-Attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fh6E-_kjr3fhoZ_-IzyZXmpGCGwi-dJk
"""

import numpy as np
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Model
from keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Attention
from keras.callbacks import ModelCheckpoint, EarlyStopping

# Download stopwords for Arabic
nltk.download('stopwords')
stop_words = set(stopwords.words('arabic'))

# Load the dataset from web
# Replace 'your_dataset_url' with the actual URL of your dataset
dfpos=pd.read_excel("/content/pos.xls",header=None,names=["Text","sentiment"])
dfneg=pd.read_excel("/content/neg.xlsx",header=None,names=["Text"])
dfneg['sentiment']=0
data = pd.concat([dfpos,dfneg],ignore_index=True)
df= data.copy()

# Preprocessing
def preprocess_text(text):
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)  # Remove URLs
    text = re.sub(r'\@\w+|\#', '', text)  # Remove mentions and hashtags
    text = re.sub('[^ุก-ู]', ' ', text)  # Remove non-Arabic characters
    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords
    return text
data['Text'] = data['Text'].fillna('').apply(str)
data['Text'] = data['Text'].apply(preprocess_text)

# Splitting into train and test sets
texts = data['Text'].tolist()
labels = data['sentiment'].tolist()
encoder = LabelEncoder()
labels = encoder.fit_transform(labels)
labels = np.expand_dims(labels, axis=1)

X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)

# Tokenization
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

vocab_size = len(tokenizer.word_index) + 1
maxlen = 100  # Max sequence length

X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

# CNN-BILSTM-Attention Model
input_layer = Input(shape=(maxlen,))
embedding_layer = Embedding(vocab_size, 100, trainable=True)(input_layer)

conv1D_layer = Conv1D(100, 3, activation='relu')(embedding_layer)
maxpooling1D_layer = MaxPooling1D(3)(conv1D_layer)

lstm_layer = Bidirectional(LSTM(128, return_sequences=True))(maxpooling1D_layer)

attention_layer = Attention()([lstm_layer, lstm_layer])

output_layer = Dense(1, activation='sigmoid')(attention_layer)

model = Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Training
checkpoint = ModelCheckpoint('modelCNNbiLSTMAttention.h5', monitor='val_loss', save_best_only=True, verbose=1)
early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)

history = model.fit(X_train, y_train, batch_size=32, epochs=30, validation_data=(X_test, y_test),
                    callbacks=[checkpoint, early_stopping])

# Evaluating the model
#model.load_weights('modelCNNbiLSTMAttention.h5')
#_, accuracy = model.evaluate(X_test, y_test)
#print('Accuracy: %.2f%%' % (accuracy * 100))

